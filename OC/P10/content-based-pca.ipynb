{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_articles, df_clicks, article_embeddings = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(article_embeddings)\n",
    "\n",
    "# Variance data\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "components = np.arange(len(cumulative_variance)) + 1\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(components, cumulative_variance, label='Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance (%)')\n",
    "plt.title('PCA Explained Variance')\n",
    "\n",
    "# Annotate specific variance percentages\n",
    "variance_thresholds = [0.9, 0.95, 0.97, 0.98, 0.99]\n",
    "for threshold in variance_thresholds:\n",
    "    component_number = np.where(cumulative_variance >= threshold)[0][0]\n",
    "    plt.scatter(component_number + 1, cumulative_variance[component_number], color='red')\n",
    "    plt.annotate(f\"{int(threshold*100)}%\", (component_number + 1, cumulative_variance[component_number]),\n",
    "                 textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.98)\n",
    "reduced_embeddings = pca.fit_transform(article_embeddings)\n",
    "# reduced_embeddings = pca.fit_transform(article_embeddings)\n",
    "print(reduced_embeddings.shape)\n",
    "article_embeddings = reduced_embeddings\n",
    "# cosine_sim_old = cosine_similarity(reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "df_articles = preprocessing_articles(df_articles)\n",
    "df_clicks = preprocessing_clicks(df_clicks)\n",
    "article_embeddings_df = pd.DataFrame(article_embeddings)\n",
    "print(\"article_embeddings shape\", article_embeddings_df.shape)\n",
    "print(\"df_articles shape\", df_articles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_clicked = df_clicks.click_article_id.value_counts().index\n",
    "df_articles = df_articles.loc[articles_clicked]\n",
    "article_embeddings_df = article_embeddings_df.loc[articles_clicked]\n",
    "\n",
    "print(\"df_articles shape\", df_articles.shape)\n",
    "print(\"article_embeddings shape\", article_embeddings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_ids = df_clicks['session_id'].unique()\n",
    "all_clicks_df = df_clicks[df_clicks['session_id'].isin(session_ids)]\n",
    "\n",
    "df_clicks = df_clicks.sample(n=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_sessions(clicks_df, test_size=0.1, val_size=0.1, random_state=42):\n",
    "    session_ids = clicks_df['session_id'].unique()\n",
    "    train_sessions, test_sessions = train_test_split(session_ids, test_size=test_size, random_state=random_state)\n",
    "    train_sessions, val_sessions = train_test_split(train_sessions, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    train_df = clicks_df[clicks_df['session_id'].isin(train_sessions)]\n",
    "    val_df = clicks_df[clicks_df['session_id'].isin(val_sessions)]\n",
    "    test_df = clicks_df[clicks_df['session_id'].isin(test_sessions)]\n",
    "    all_df = clicks_df[clicks_df['session_id'].isin(session_ids)]\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    " \n",
    "\n",
    "# Split the clicks dataframe\n",
    "train_clicks_df, val_clicks_df, test_clicks_df = train_test_split_sessions(df_clicks)\n",
    "\n",
    "print(f\"Training clicks shape: {train_clicks_df.shape}\")\n",
    "print(f\"Validation clicks shape: {val_clicks_df.shape}\")\n",
    "print(f\"Testing clicks shape: {test_clicks_df.shape}\")\n",
    "print(f\"All clicks shape: {all_clicks_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merging Articles Embeddings with Articles Metadata\n",
    "\n",
    "# Merging with articles_metadata\n",
    "# articles_merged_df = pd.merge(df_articles, article_embeddings_df, on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def create_user_profiles(clicks_df, article_embeddings_df):\n",
    "    user_profiles = clicks_df.groupby('user_id')['click_article_id'].apply(list).reset_index()\n",
    "    embeddings_dict = article_embeddings_df.T.to_dict('list')\n",
    "    \n",
    "    user_profiles['user_embedding'] = user_profiles['click_article_id'].progress_apply(\n",
    "        lambda x: np.mean([embeddings_dict[article] for article in x if article in embeddings_dict], axis=0)\n",
    "    )\n",
    "    \n",
    "    return user_profiles\n",
    "\n",
    "user_profiles_df_train = create_user_profiles(train_clicks_df, article_embeddings_df)\n",
    "user_profiles_df_test = create_user_profiles(test_clicks_df, article_embeddings_df)\n",
    "user_profiles_df_val = create_user_profiles(val_clicks_df, article_embeddings_df)\n",
    "user_profiles_df_all = create_user_profiles(all_clicks_df, article_embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_content_based_model(input_dim):\n",
    "    model = models.Sequential()\n",
    "    # Input Layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    \n",
    "    # Hidden Layers\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # Output Layer - Predicting the relevance score\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[ndcg_5, ndcg_10, mean_mrr])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_data(user_profiles_df_train, articles_df, articles_embeddings_df):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    embeddings_dict = articles_embeddings_df.T.to_dict('list')\n",
    "    \n",
    "    for i, user in tqdm(user_profiles_df_train.iterrows(), total=len(user_profiles_df_train)):\n",
    "        # if i >= 50000:\n",
    "        #     break\n",
    "        \n",
    "        user_embedding = user['user_embedding']\n",
    "        clicked_articles = user['click_article_id']\n",
    "        \n",
    "        for article_id in clicked_articles:\n",
    "            if article_id in embeddings_dict:\n",
    "                article_embedding = embeddings_dict[article_id]\n",
    "                combined_features = np.concatenate((user_embedding, article_embedding))\n",
    "                X.append(combined_features)\n",
    "                y.append(1) # Positive sample\n",
    "        \n",
    "        # Add some negative samples for training\n",
    "        negative_samples = articles_df[~articles_df['article_id'].isin(clicked_articles)]['article_id'].sample(n=len(clicked_articles))\n",
    "        \n",
    "        for article_id in negative_samples:\n",
    "            if article_id in embeddings_dict:\n",
    "                article_embedding = embeddings_dict[article_id]\n",
    "                combined_features = np.concatenate((user_embedding, article_embedding))\n",
    "                X.append(combined_features)\n",
    "                y.append(0) # Negative sample\n",
    "                \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(user_profiles_df_train, df_articles, article_embeddings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = prepare_data(user_profiles_df_val, df_articles, article_embeddings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(labels, predictions):\n",
    "    if len(labels) != len(predictions):\n",
    "        raise ValueError(\"Length of labels and predictions must be equal\")\n",
    "\n",
    "    # Combine labels and predictions, then sort by prediction score in descending order\n",
    "    combined = list(zip(labels, predictions))\n",
    "    combined_sorted = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Identify the rank position of the first relevant item (label == 1)\n",
    "    for idx, (label, _) in enumerate(combined_sorted):\n",
    "        if label == 1:\n",
    "            return 1.0 / (idx + 1)\n",
    "\n",
    "    # If no relevant item is found, return 0\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming article_embeddings's second dimension size is 250\n",
    "input_dim = X_train.shape[1]\n",
    "content_based_model = create_content_based_model(input_dim)\n",
    "\n",
    "content_based_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f\"\\n\\nEpoch {epoch+1}:\", end=\" \")\n",
    "        for key, value in logs.items():\n",
    "            print(f\"\\n- {key}: {value:.4f}\", end=\", \")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Using the custom callback\n",
    "custom_metrics_callback = CustomMetricsCallback()\n",
    "# Train the model\n",
    "history = content_based_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[custom_metrics_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = article_embeddings_df.T.to_dict('list')\n",
    "model = content_based_model\n",
    "articles_df = df_articles\n",
    "\n",
    "# List to store evaluation metrics\n",
    "precisions, recalls, mrrs, ndcgs = [], [], [], []\n",
    "\n",
    "all_true_labels = []\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_user_wrapper(args):\n",
    "    return process_user(*args)\n",
    "\n",
    "def process_user(user, k):\n",
    "    user_embedding = user['user_embedding']\n",
    "    user_id = user['user_id']\n",
    "    clicked_articles = set(user['click_article_id'])\n",
    "\n",
    "    all_embeddings = []\n",
    "    article_ids = [article_id for article_id in articles_df['article_id'] if article_id in embeddings_dict]\n",
    "    all_embeddings = [np.concatenate((user_embedding, embeddings_dict[article_id])).reshape(1, -1) for article_id in article_ids]\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    scores = model.predict(all_embeddings, verbose=0).flatten()\n",
    "    \n",
    "    true_labels = np.array([1 if article_id in clicked_articles else 0 for article_id in article_ids])\n",
    "\n",
    "    precisions.append(precision_at_k(true_labels, scores, k))\n",
    "    recalls.append(recall_at_k(true_labels, scores, k))\n",
    "    mrrs.append(mrr(true_labels, scores))\n",
    "    ndcgs.append(ndcg_at_k(true_labels, scores, k))\n",
    "\n",
    "    all_true_labels.extend(true_labels)\n",
    "    all_scores.extend(scores)\n",
    "    \n",
    "def evaluate_model_optimized(model, user_profiles_df_train, articles_df, articles_embeddings_df, k=10, num_users=2000):\n",
    "    sampled_user_profiles_df = user_profiles_df_train.sample(n=num_users, random_state=42)\n",
    "    \n",
    "    # sampled_user_profiles_df = user_profiles_df_train[user_profiles_df_train[\"user_id\"] == 15958]\n",
    "\n",
    "    for _, user in tqdm(sampled_user_profiles_df.iterrows(), total=num_users, desc=\"Evaluating\", ncols=100):\n",
    "        process_user(user, k)\n",
    "\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_mrr = np.mean(mrrs)\n",
    "    avg_ndcg = np.mean(ndcgs)\n",
    "    rmse = np.sqrt(mean_squared_error(all_true_labels, all_scores))\n",
    "    mae = mean_absolute_error(all_true_labels, all_scores)\n",
    "    \n",
    "    return avg_ndcg, avg_mrr, avg_precision, avg_recall, rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles_df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ndcg, avg_mrr, avg_precision, avg_recall, rmse, mae = evaluate_model_optimized(content_based_model, user_profiles_df_test, df_articles, article_embeddings_df, k=10, num_users=10)\n",
    "print(f\"NDCG@10: {avg_ndcg:.4f}, MRR: {avg_mrr:.4f}, precision: {avg_precision:.4f}, rmse: {rmse:.4f} , mae: {mae:.4f}, recall: {avg_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ndcg_score, mrr_score, auc_score, y_pred = evaluate_model_on_test_data(content_based_model, X_test, y_test)\n",
    "# print(f\"NDCG@10: {ndcg_score:.4f}, MRR: {mrr_score:.4f}, AUC: {auc_score:.4f}\")\n",
    "# Evaluation\n",
    "# ndcg_score, mrr_score, g_auc_score, y_true, y_pred, user_ids = evaluate_model_optimized(content_based_model, user_profiles_df_test, df_articles, article_embeddings_df, k=10, num_users=len(user_profiles_df_test))\n",
    "avg_ndcg, avg_mrr, avg_precision, avg_recall, rmse, mae = evaluate_model_optimized(content_based_model, user_profiles_df_test, df_articles, article_embeddings_df, k=10, num_users=10)\n",
    "print(f\"NDCG@10: {avg_ndcg:.4f}, MRR: {avg_mrr:.4f}, precision: {avg_precision:.4f}, rmse: {rmse:.4f} , mae: {mae:.4f}, recall: {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_top_k_articles(user_id, user_profiles_df, df_articles, article_embeddings_df, model, k=5):\n",
    "    tmp_df_articles = df_articles.copy()\n",
    "    # Retrieve the user's embedding\n",
    "    user_profile = user_profiles_df[user_profiles_df['user_id'] == user_id].iloc[0]\n",
    "    \n",
    "    if user_profile.empty:\n",
    "        raise ValueError(\"User ID not found in the user profiles.\")\n",
    "\n",
    "    user_embedding = user_profile['user_embedding']\n",
    "\n",
    "    # Get all articles embeddings\n",
    "    embeddings_dict = article_embeddings_df.T.to_dict('list')\n",
    "    \n",
    "    article_ids = []\n",
    "    combined_features_list = []\n",
    "    \n",
    "    for article_id, article_embedding in embeddings_dict.items():\n",
    "        article_ids.append(article_id)\n",
    "        combined_features = np.concatenate((user_embedding, article_embedding)).reshape(1, -1)\n",
    "        combined_features_list.append(combined_features)\n",
    "\n",
    "    all_embeddings = np.vstack(combined_features_list)\n",
    "    \n",
    "    # Predict relevance scores using the trained model\n",
    "    scores = model.predict(all_embeddings, verbose=0).flatten()\n",
    "\n",
    "    print(user_profile[\"click_article_id\"])\n",
    "    # Add scores to dataframe\n",
    "    tmp_df_articles['score'] = tmp_df_articles['article_id'].map(dict(zip(article_ids, scores)))\n",
    "    tmp_df_articles = tmp_df_articles.sort_values(by='score', ascending=False)\n",
    "\n",
    "    top_articles = tmp_df_articles.copy()[[\"article_id\",\"category_id\",\"score\"]]\n",
    "    user_article_clicked = top_articles[top_articles['article_id'].isin(user_profile[\"click_article_id\"])].reset_index(drop=True)\n",
    "\n",
    "    top_articles = top_articles[~top_articles['article_id'].isin(user_profile[\"click_article_id\"])]\n",
    "\n",
    "    # Rank articles based on scores\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "    top_k_article_ids = [article_ids[i] for i in top_k_indices]\n",
    "    \n",
    "    # Rank articles based on scores (worst)\n",
    "    bottom_k_indices = np.argsort(scores)[:k]\n",
    "    bottom_k_article_ids = [article_ids[i] for i in bottom_k_indices]\n",
    "\n",
    "    # Fetch top K articles metadata\n",
    "    top_k_articles = top_articles[top_articles['article_id'].isin(top_k_article_ids)].reset_index(drop=True)\n",
    "    bottom_k_article_ids = top_articles[top_articles['article_id'].isin(bottom_k_article_ids)].reset_index(drop=True)\n",
    "    bottom_k_article_ids = bottom_k_article_ids.sort_values(by='score', ascending=True)\n",
    "    \n",
    "    # Display the top K articles usi\n",
    "    return top_k_articles, bottom_k_article_ids, user_article_clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_dcg(y_true, y_pred, k):\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(1, len(y_true) + 1) + 1)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def compute_ndcg(y_true, y_pred, k):\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return 0.0\n",
    "    dcg = compute_dcg(y_true, y_pred, k)\n",
    "    idcg = compute_dcg(y_true, sorted(y_true, reverse=True), k)\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def show_ndcg_for_user(user_id, k=10):\n",
    "    user = sampled_user_profiles_df.loc[sampled_user_profiles_df['user_id'] == user_id].iloc[0]\n",
    "    user_embedding = user['user_embedding']\n",
    "    clicked_articles = set(user['click_article_id'])\n",
    "\n",
    "    all_embeddings = []\n",
    "    article_ids = [article_id for article_id in articles_df['article_id'] if article_id in embeddings_dict]\n",
    "    all_embeddings = [np.concatenate((user_embedding, embeddings_dict[article_id])).reshape(1, -1) for article_id in article_ids]\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    scores = model.predict(all_embeddings, verbose=0).flatten()\n",
    "    \n",
    "    true_labels = np.array([1 if article_id in clicked_articles else 0 for article_id in article_ids])\n",
    "    \n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    ranked_article_ids = np.array(article_ids)[order]\n",
    "    ranked_scores = scores[order]\n",
    "    ranked_true_labels = true_labels[order]\n",
    "      \n",
    "    ndcg_score = compute_ndcg(ranked_true_labels, ranked_scores, k)\n",
    "    \n",
    "    print(f\"User ID: {user_id}\")\n",
    "    \n",
    "    print(\"\\nGround Truth Relevance:\")\n",
    "    for article_id, label in zip(article_ids, true_labels):\n",
    "        if label > 0:\n",
    "            print(f\"  Article {article_id}: Relevance {label}\")\n",
    "    \n",
    "    print(\"\\nTop-{0} Predicted Ranking:\".format(k))\n",
    "    for i, (article_id, score, true_label) in enumerate(zip(ranked_article_ids, ranked_scores, ranked_true_labels)):\n",
    "        print(f\"  Rank {i+1}: Article {article_id} | Predicted Score: {score:.4f} | True Relevance: {true_label}\")\n",
    "    \n",
    "    print(f\"\\nNDCG@{k}: {ndcg_score:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "sampled_user_profiles_df = user_profiles_df_test.sample(n=100, random_state=42)\n",
    "# evaluate_model_optimized(model, user_profiles_df_train, articles_df, articles_embeddings_df, k=10, num_users=2000)\n",
    "# Replace 'sample_user_id' with an actual user ID from your dataset\n",
    "# sampled_user_profiles_df\n",
    "# show_ndcg_for_user(15958, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detailed_ndcg_for_user(user_id, k=10):\n",
    "    user = sampled_user_profiles_df.loc[sampled_user_profiles_df['user_id'] == user_id].iloc[0]\n",
    "    user_embedding = user['user_embedding']\n",
    "    clicked_articles = set(user['click_article_id'])\n",
    "\n",
    "    all_embeddings = []\n",
    "    article_ids = [article_id for article_id in articles_df['article_id'] if article_id in embeddings_dict]\n",
    "    all_embeddings = [np.concatenate((user_embedding, embeddings_dict[article_id])).reshape(1, -1) for article_id in article_ids]\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    scores = model.predict(all_embeddings, verbose=0).flatten()\n",
    "    \n",
    "    true_labels = np.array([1 if article_id in clicked_articles else 0 for article_id in article_ids])\n",
    "    \n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    ranked_article_ids = np.array(article_ids)[order]\n",
    "    ranked_scores = scores[order]\n",
    "    ranked_true_labels = true_labels[order]\n",
    "    \n",
    "    dcg_score = compute_dcg(ranked_true_labels, ranked_scores, k)\n",
    "    idcg_score = compute_dcg(ranked_true_labels, sorted(ranked_true_labels, reverse=True), k)\n",
    "    ndcg_score = dcg_score / idcg_score if idcg_score > 0 else 0.0\n",
    "    \n",
    "    print(f\"User ID: {user_id}\")\n",
    "\n",
    "    print(\"\\nGround Truth Relevance:\")\n",
    "    for article_id, label in zip(article_ids, true_labels):\n",
    "        if label > 0:\n",
    "            print(f\"  Article {article_id}: Relevance {label}\")\n",
    "    \n",
    "    print(\"\\nTop-{0} Predicted Ranking:\".format(k))\n",
    "    for i, (article_id, score, true_label) in enumerate(zip(ranked_article_ids, ranked_scores, ranked_true_labels)):\n",
    "        print(f\"  Rank {i+1}: Article {article_id} | Predicted Score: {score:.4f} | True Relevance: {true_label}\")\n",
    "    \n",
    "    print(f\"\\nDCG@{k}: {dcg_score:.4f}\")\n",
    "    print(f\"IDCG@{k}: {idcg_score:.4f}\")\n",
    "    print(f\"NDCG@{k}: {ndcg_score:.4f}\")\n",
    "    \n",
    "# show_detailed_ndcg_for_user(15958, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detailed_ndcg_for_user(user_id, k=10):\n",
    "    user = sampled_user_profiles_df.loc[sampled_user_profiles_df['user_id'] == user_id].iloc[0]\n",
    "    user_embedding = user['user_embedding']\n",
    "    clicked_articles = set(user['click_article_id'])\n",
    "\n",
    "    all_embeddings = []\n",
    "    article_ids = [article_id for article_id in articles_df['article_id'] if article_id in embeddings_dict]\n",
    "    all_embeddings = [np.concatenate((user_embedding, embeddings_dict[article_id])).reshape(1, -1) for article_id in article_ids]\n",
    "    \n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    scores = model.predict(all_embeddings, verbose=0).flatten()\n",
    "    \n",
    "    true_labels = np.array([1 if article_id in clicked_articles else 0 for article_id in article_ids])\n",
    "    \n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    ranked_article_ids = np.array(article_ids)[order]\n",
    "    ranked_scores = scores[order]\n",
    "    ranked_true_labels = true_labels[order]\n",
    "    \n",
    "    dcg_score = compute_dcg(ranked_true_labels, ranked_scores, k)\n",
    "    idcg_score = compute_dcg(ranked_true_labels, sorted(ranked_true_labels, reverse=True), k)\n",
    "    ndcg_score = dcg_score / idcg_score if idcg_score > 0 else 0.0\n",
    "    \n",
    "    print(f\"User ID: {user_id}\")\n",
    "\n",
    "    print(\"\\nGround Truth Relevance:\")\n",
    "    for article_id, label in zip(article_ids, true_labels):\n",
    "        if label > 0:\n",
    "            print(f\"  Article {article_id}: Relevance {label}\")\n",
    "    \n",
    "    print(\"\\nTop-{0} Predicted Ranking:\".format(k))\n",
    "    for i, (article_id, score, true_label) in enumerate(zip(ranked_article_ids, ranked_scores, ranked_true_labels)):\n",
    "        print(f\"  Rank {i+1}: Article {article_id} | Predicted Score: {score:.4f} | True Relevance: {true_label}\")\n",
    "    \n",
    "    print(f\"\\nDCG@{k}: {dcg_score:.4f}\")\n",
    "    print(f\"IDCG@{k}: {idcg_score:.4f}\")\n",
    "    print(f\"NDCG@{k}: {ndcg_score:.4f}\")\n",
    "# show_detailed_ndcg_for_user(15958, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_detailed_ndcg_for_user(280171, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_user_profiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_articles, bottom_k_article_ids, user_article_clicked = infer_top_k_articles(user_id, user_profiles_df_all, df_articles, article_embeddings_df, content_based_model, k=5)user_id=3\n",
    "user_id=15958\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_article_clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_articles[top_k_articles[\"article_id\"].isin(user_article_clicked[\"article_id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_k_article_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_based_model.save('./output/content-based-reduced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_profiles_df_test.to_pickle(\"./output/user_profiles_df_test-reduced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/embeddings_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrames to disk\n",
    "user_profiles_df_all.to_pickle(\"./output/user_profiles_df_all-reduced.pkl\")\n",
    "df_articles.to_pickle(\"./output/df_articles-reduced.pkl\")\n",
    "article_embeddings_df.to_pickle(\"./output/article_embeddings_df-reduced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles[[\"article_id\",\"category_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_all_articles_scores(user_id, df, df_articles, article_embeddings_df, model):\n",
    "    # Retrieve the user's embedding\n",
    "    user_profile = df[df['user_id'] == user_id].iloc[0]\n",
    "    \n",
    "    if user_profile.empty:\n",
    "        raise ValueError(\"User ID not found in the user profiles.\")\n",
    "\n",
    "    user_embedding = user_profile['user_embedding']\n",
    "\n",
    "    # Get all articles embeddings\n",
    "    embeddings_dict = article_embeddings_df.T.to_dict('list')\n",
    "    \n",
    "    article_ids = list(embeddings_dict.keys())\n",
    "    combined_features_list = [np.concatenate((user_embedding, article_embedding)).reshape(1, -1) \n",
    "                              for article_embedding in embeddings_dict.values()]\n",
    "\n",
    "    all_embeddings = np.vstack(combined_features_list)\n",
    "    \n",
    "    # Predict relevance scores using the trained model\n",
    "    scores = model.predict(all_embeddings, verbose=0).flatten()\n",
    "\n",
    "    # Create a dataframe with article IDs, category IDs, and scores\n",
    "    article_scores_df = df_articles[['article_id', 'category_id']].copy()\n",
    "    article_scores_df['score'] = article_scores_df['article_id'].map(dict(zip(article_ids, scores)))\n",
    "    \n",
    "    # Remove any unwanted header rows if present\n",
    "    # article_scores_df.columns = article_scores_df.columns.droplevel(0)\n",
    "    article_scores_df.reset_index(drop=True, inplace=True)\n",
    "    return article_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_scores = infer_all_articles_scores(user_id, user_profiles_df_all, df_articles, article_embeddings_df, content_based_model)\n",
    "articles_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result = {int(k): int(v) for k, v in df_articles[[\"article_id\", \"category_id\"]].values}\n",
    "with open('articles.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
