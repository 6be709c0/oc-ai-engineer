{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_articles, df_clicks, article_embeddings = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_articles shape (364047, 5)\n",
      "article_embeddings shape (364047, 250)\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "df_articles = preprocessing_articles(df_articles)\n",
    "df_clicks = preprocessing_clicks(df_clicks)\n",
    "article_embeddings_df = pd.DataFrame(article_embeddings)\n",
    "\n",
    "print(\"df_articles shape\", df_articles.shape)\n",
    "print(\"article_embeddings shape\", article_embeddings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_clicked = df_clicks.click_article_id.value_counts().index\n",
    "df_articles = df_articles.loc[articles_clicked]\n",
    "article_embeddings_df = article_embeddings_df.loc[articles_clicked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_articles shape (46033, 5)\n",
      "article_embeddings shape (46033, 250)\n"
     ]
    }
   ],
   "source": [
    "print(\"df_articles shape\", df_articles.shape)\n",
    "print(\"article_embeddings shape\", article_embeddings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training clicks shape: (2419742, 14)\n",
      "Validation clicks shape: (269559, 14)\n",
      "Testing clicks shape: (298880, 14)\n"
     ]
    }
   ],
   "source": [
    "def train_test_split_sessions(clicks_df, test_size=0.1, val_size=0.1, random_state=42):\n",
    "    session_ids = clicks_df['session_id'].unique()\n",
    "    train_sessions, test_sessions = train_test_split(session_ids, test_size=test_size, random_state=random_state)\n",
    "    train_sessions, val_sessions = train_test_split(train_sessions, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    train_df = clicks_df[clicks_df['session_id'].isin(train_sessions)]\n",
    "    val_df = clicks_df[clicks_df['session_id'].isin(val_sessions)]\n",
    "    test_df = clicks_df[clicks_df['session_id'].isin(test_sessions)]\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    " \n",
    "\n",
    "# Split the clicks dataframe\n",
    "train_clicks_df, val_clicks_df, test_clicks_df = train_test_split_sessions(df_clicks)\n",
    "\n",
    "print(f\"Training clicks shape: {train_clicks_df.shape}\")\n",
    "print(f\"Validation clicks shape: {val_clicks_df.shape}\")\n",
    "print(f\"Testing clicks shape: {test_clicks_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merging Articles Embeddings with Articles Metadata\n",
    "\n",
    "# Merging with articles_metadata\n",
    "# articles_merged_df = pd.merge(df_articles, article_embeddings_df, on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294662/294662 [00:16<00:00, 17576.27it/s]\n",
      "100%|██████████| 80449/80449 [00:02<00:00, 32352.32it/s]\n",
      "100%|██████████| 74242/74242 [00:02<00:00, 34275.63it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def create_user_profiles(clicks_df, article_embeddings_df):\n",
    "    user_profiles = clicks_df.groupby('user_id')['click_article_id'].apply(list).reset_index()\n",
    "    embeddings_dict = article_embeddings_df.T.to_dict('list')\n",
    "    \n",
    "    user_profiles['user_embedding'] = user_profiles['click_article_id'].progress_apply(\n",
    "        lambda x: np.mean([embeddings_dict[article] for article in x if article in embeddings_dict], axis=0)\n",
    "    )\n",
    "    \n",
    "    return user_profiles\n",
    "\n",
    "user_profiles_df = create_user_profiles(train_clicks_df, article_embeddings_df)\n",
    "user_profiles_df_test = create_user_profiles(test_clicks_df, article_embeddings_df)\n",
    "user_profiles_df_val = create_user_profiles(val_clicks_df, article_embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_content_based_model(input_dim):\n",
    "    model = models.Sequential()\n",
    "    # Input Layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    \n",
    "    # Hidden Layers\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # Output Layer - Predicting the relevance score\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[ndcg_5, ndcg_10, mean_mrr, g_auc])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_data(user_profiles_df, articles_df, articles_embeddings_df):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    embeddings_dict = articles_embeddings_df.T.to_dict('list')\n",
    "    \n",
    "    for i, user in tqdm(user_profiles_df.iterrows(), total=len(user_profiles_df)):\n",
    "        if i >= 50000:\n",
    "            break\n",
    "        \n",
    "        user_embedding = user['user_embedding']\n",
    "        clicked_articles = user['click_article_id']\n",
    "        \n",
    "        for article_id in clicked_articles:\n",
    "            if article_id in embeddings_dict:\n",
    "                article_embedding = embeddings_dict[article_id]\n",
    "                combined_features = np.concatenate((user_embedding, article_embedding))\n",
    "                X.append(combined_features)\n",
    "                y.append(1) # Positive sample\n",
    "        \n",
    "        # Add some negative samples for training\n",
    "        negative_samples = articles_df[~articles_df['article_id'].isin(clicked_articles)]['article_id'].sample(n=len(clicked_articles))\n",
    "        \n",
    "        for article_id in negative_samples:\n",
    "            if article_id in embeddings_dict:\n",
    "                article_embedding = embeddings_dict[article_id]\n",
    "                combined_features = np.concatenate((user_embedding, article_embedding))\n",
    "                X.append(combined_features)\n",
    "                y.append(0) # Negative sample\n",
    "                \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 50000/294662 [01:33<07:38, 533.82it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = prepare_data(user_profiles_df, df_articles, article_embeddings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 50000/74242 [01:16<00:36, 655.26it/s]\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = prepare_data(user_profiles_df_val, df_articles, article_embeddings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.backend as K\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# def precision_at_k(true_labels, pred_scores, k=5):\n",
    "#     top_k_indices = np.argsort(pred_scores)[-k:]\n",
    "#     top_k_true_labels = true_labels[top_k_indices]\n",
    "#     return np.sum(top_k_true_labels) / k\n",
    "\n",
    "# def recall_at_k(true_labels, pred_scores, k=5):\n",
    "#     top_k_indices = np.argsort(pred_scores)[-k:]\n",
    "#     top_k_true_labels = true_labels[top_k_indices]\n",
    "#     return np.sum(top_k_true_labels) / np.sum(true_labels)\n",
    "\n",
    "def mrr(labels, predictions):\n",
    "    if len(labels) != len(predictions):\n",
    "        raise ValueError(\"Length of labels and predictions must be equal\")\n",
    "\n",
    "    # Combine labels and predictions, then sort by prediction score in descending order\n",
    "    combined = list(zip(labels, predictions))\n",
    "    combined_sorted = sorted(combined, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Identify the rank position of the first relevant item (label == 1)\n",
    "    for idx, (label, _) in enumerate(combined_sorted):\n",
    "        if label == 1:\n",
    "            return 1.0 / (idx + 1)\n",
    "\n",
    "    # If no relevant item is found, return 0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# def ndcg_at_k(y_true, y_pred, k=5):\n",
    "#     def compute_dcg(y_true, y_pred, k):\n",
    "#         order = np.argsort(y_pred)[::-1]\n",
    "#         y_true = np.take(y_true, order[:k])\n",
    "#         gains = 2 ** y_true - 1\n",
    "#         discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "#         return np.sum(gains / discounts)\n",
    "\n",
    "#     def compute_ndcg(y_true, y_pred, k):\n",
    "#         dcg = compute_dcg(y_true, y_pred, k)\n",
    "#         ideal_dcg = compute_dcg(y_true, y_true, k)  # Ideal sorted DCG\n",
    "#         return dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "#     return tf.py_function(compute_ndcg, (y_true, y_pred, k), tf.double)\n",
    "\n",
    "# def g_auc(y_true, y_pred, user_ids):\n",
    "#     def compute_auc(y_true, y_pred, user_ids):\n",
    "#         users = np.unique(user_ids)\n",
    "#         aucs = []\n",
    "#         for user in users:\n",
    "#             user_indices = np.where(user_ids == user)[0]\n",
    "#             user_indices = tf.constant(user_indices, dtype=tf.int32)\n",
    "            \n",
    "#             user_true = tf.gather(y_true, user_indices)\n",
    "#             user_pred = tf.gather(y_pred, user_indices)\n",
    "            \n",
    "#             user_true_np = user_true.numpy()\n",
    "#             user_pred_np = user_pred.numpy()\n",
    "\n",
    "#             if len(np.unique(user_true_np)) > 1:  # Avoid cases where true labels are all the same\n",
    "#                 auc = roc_auc_score(user_true_np, user_pred_np)\n",
    "#                 aucs.append(auc)\n",
    "#         return np.mean(aucs) if aucs else 0.\n",
    "\n",
    "#     return tf.py_function(compute_auc, (y_true, y_pred, user_ids), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               64128     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72449 (283.00 KB)\n",
      "Trainable params: 72449 (283.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Assuming article_embeddings's second dimension size is 250\n",
    "input_dim = X_train.shape[1]\n",
    "content_based_model = create_content_based_model(input_dim)\n",
    "\n",
    "content_based_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21033628, -0.96357318, -0.19693483, ..., -0.4184863 ,\n",
       "         0.1679776 ,  0.27869353],\n",
       "       [-0.21033628, -0.96357318, -0.19693483, ..., -0.39606935,\n",
       "         0.30193529,  0.48606798],\n",
       "       [-0.21033628, -0.96357318, -0.19693483, ..., -0.0688789 ,\n",
       "         0.24662791, -0.00772025],\n",
       "       ...,\n",
       "       [ 0.09646962, -0.97281748, -0.18153932, ..., -0.38751572,\n",
       "        -0.02546285,  0.24012679],\n",
       "       [ 0.09646962, -0.97281748, -0.18153932, ..., -0.86725497,\n",
       "         0.44057399, -0.03573098],\n",
       "       [ 0.09646962, -0.97281748, -0.18153932, ...,  0.63440394,\n",
       "         0.31899649,  0.24804382]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49047/49080 [============================>.] - ETA: 0s - loss: 0.2840 - ndcg_5: 0.4977 - ndcg_10: 0.4977 - mean_mrr: 0.0631 - g_auc: 0.9500\n",
      "\n",
      "Epoch 1: \n",
      "- loss: 0.2840, \n",
      "- ndcg_5: 0.4978, \n",
      "- ndcg_10: 0.4978, \n",
      "- mean_mrr: 0.0631, \n",
      "- g_auc: 0.9500, \n",
      "- val_loss: 0.1390, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9805, \n",
      "\n",
      "49080/49080 [==============================] - 77s 2ms/step - loss: 0.2840 - ndcg_5: 0.4978 - ndcg_10: 0.4978 - mean_mrr: 0.0631 - g_auc: 0.9500 - val_loss: 0.1390 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9805\n",
      "Epoch 2/10\n",
      "49060/49080 [============================>.] - ETA: 0s - loss: 0.2313 - ndcg_5: 0.4984 - ndcg_10: 0.4984 - mean_mrr: 0.0632 - g_auc: 0.9677\n",
      "\n",
      "Epoch 2: \n",
      "- loss: 0.2313, \n",
      "- ndcg_5: 0.4984, \n",
      "- ndcg_10: 0.4984, \n",
      "- mean_mrr: 0.0632, \n",
      "- g_auc: 0.9677, \n",
      "- val_loss: 0.1254, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9828, \n",
      "\n",
      "49080/49080 [==============================] - 77s 2ms/step - loss: 0.2313 - ndcg_5: 0.4984 - ndcg_10: 0.4984 - mean_mrr: 0.0632 - g_auc: 0.9677 - val_loss: 0.1254 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9828\n",
      "Epoch 3/10\n",
      "49050/49080 [============================>.] - ETA: 0s - loss: 0.2161 - ndcg_5: 0.5020 - ndcg_10: 0.5020 - mean_mrr: 0.0636 - g_auc: 0.9717\n",
      "\n",
      "Epoch 3: \n",
      "- loss: 0.2161, \n",
      "- ndcg_5: 0.5020, \n",
      "- ndcg_10: 0.5020, \n",
      "- mean_mrr: 0.0636, \n",
      "- g_auc: 0.9717, \n",
      "- val_loss: 0.1114, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9838, \n",
      "\n",
      "49080/49080 [==============================] - 76s 2ms/step - loss: 0.2161 - ndcg_5: 0.5020 - ndcg_10: 0.5020 - mean_mrr: 0.0636 - g_auc: 0.9717 - val_loss: 0.1114 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9838\n",
      "Epoch 4/10\n",
      "49073/49080 [============================>.] - ETA: 0s - loss: 0.2072 - ndcg_5: 0.5040 - ndcg_10: 0.5040 - mean_mrr: 0.0639 - g_auc: 0.9739\n",
      "\n",
      "Epoch 4: \n",
      "- loss: 0.2072, \n",
      "- ndcg_5: 0.5040, \n",
      "- ndcg_10: 0.5040, \n",
      "- mean_mrr: 0.0639, \n",
      "- g_auc: 0.9739, \n",
      "- val_loss: 0.1072, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9844, \n",
      "\n",
      "49080/49080 [==============================] - 77s 2ms/step - loss: 0.2072 - ndcg_5: 0.5040 - ndcg_10: 0.5040 - mean_mrr: 0.0639 - g_auc: 0.9739 - val_loss: 0.1072 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9844\n",
      "Epoch 5/10\n",
      "49076/49080 [============================>.] - ETA: 0s - loss: 0.2015 - ndcg_5: 0.4990 - ndcg_10: 0.4990 - mean_mrr: 0.0633 - g_auc: 0.9753\n",
      "\n",
      "Epoch 5: \n",
      "- loss: 0.2015, \n",
      "- ndcg_5: 0.4990, \n",
      "- ndcg_10: 0.4990, \n",
      "- mean_mrr: 0.0633, \n",
      "- g_auc: 0.9753, \n",
      "- val_loss: 0.1042, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9848, \n",
      "\n",
      "49080/49080 [==============================] - 76s 2ms/step - loss: 0.2015 - ndcg_5: 0.4990 - ndcg_10: 0.4990 - mean_mrr: 0.0633 - g_auc: 0.9753 - val_loss: 0.1042 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9848\n",
      "Epoch 6/10\n",
      "49056/49080 [============================>.] - ETA: 0s - loss: 0.1968 - ndcg_5: 0.5019 - ndcg_10: 0.5019 - mean_mrr: 0.0636 - g_auc: 0.9764\n",
      "\n",
      "Epoch 6: \n",
      "- loss: 0.1968, \n",
      "- ndcg_5: 0.5020, \n",
      "- ndcg_10: 0.5020, \n",
      "- mean_mrr: 0.0636, \n",
      "- g_auc: 0.9765, \n",
      "- val_loss: 0.0998, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9852, \n",
      "\n",
      "49080/49080 [==============================] - 77s 2ms/step - loss: 0.1968 - ndcg_5: 0.5020 - ndcg_10: 0.5020 - mean_mrr: 0.0636 - g_auc: 0.9765 - val_loss: 0.0998 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9852\n",
      "Epoch 7/10\n",
      "49076/49080 [============================>.] - ETA: 0s - loss: 0.1932 - ndcg_5: 0.5004 - ndcg_10: 0.5004 - mean_mrr: 0.0634 - g_auc: 0.9773\n",
      "\n",
      "Epoch 7: \n",
      "- loss: 0.1932, \n",
      "- ndcg_5: 0.5003, \n",
      "- ndcg_10: 0.5003, \n",
      "- mean_mrr: 0.0634, \n",
      "- g_auc: 0.9773, \n",
      "- val_loss: 0.1004, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9853, \n",
      "\n",
      "49080/49080 [==============================] - 83s 2ms/step - loss: 0.1932 - ndcg_5: 0.5003 - ndcg_10: 0.5003 - mean_mrr: 0.0634 - g_auc: 0.9773 - val_loss: 0.1004 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9853\n",
      "Epoch 8/10\n",
      "49072/49080 [============================>.] - ETA: 0s - loss: 0.1898 - ndcg_5: 0.4968 - ndcg_10: 0.4968 - mean_mrr: 0.0630 - g_auc: 0.9780\n",
      "\n",
      "Epoch 8: \n",
      "- loss: 0.1898, \n",
      "- ndcg_5: 0.4968, \n",
      "- ndcg_10: 0.4968, \n",
      "- mean_mrr: 0.0630, \n",
      "- g_auc: 0.9780, \n",
      "- val_loss: 0.1072, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9855, \n",
      "\n",
      "49080/49080 [==============================] - 78s 2ms/step - loss: 0.1898 - ndcg_5: 0.4968 - ndcg_10: 0.4968 - mean_mrr: 0.0630 - g_auc: 0.9780 - val_loss: 0.1072 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9855\n",
      "Epoch 9/10\n",
      "49056/49080 [============================>.] - ETA: 0s - loss: 0.1874 - ndcg_5: 0.4993 - ndcg_10: 0.4993 - mean_mrr: 0.0633 - g_auc: 0.9786\n",
      "\n",
      "Epoch 9: \n",
      "- loss: 0.1874, \n",
      "- ndcg_5: 0.4993, \n",
      "- ndcg_10: 0.4993, \n",
      "- mean_mrr: 0.0633, \n",
      "- g_auc: 0.9786, \n",
      "- val_loss: 0.0973, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9858, \n",
      "\n",
      "49080/49080 [==============================] - 91s 2ms/step - loss: 0.1874 - ndcg_5: 0.4993 - ndcg_10: 0.4993 - mean_mrr: 0.0633 - g_auc: 0.9786 - val_loss: 0.0973 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9858\n",
      "Epoch 10/10\n",
      "49075/49080 [============================>.] - ETA: 0s - loss: 0.1852 - ndcg_5: 0.4970 - ndcg_10: 0.4970 - mean_mrr: 0.0630 - g_auc: 0.9790\n",
      "\n",
      "Epoch 10: \n",
      "- loss: 0.1852, \n",
      "- ndcg_5: 0.4970, \n",
      "- ndcg_10: 0.4970, \n",
      "- mean_mrr: 0.0630, \n",
      "- g_auc: 0.9790, \n",
      "- val_loss: 0.0962, \n",
      "- val_ndcg_5: 0.5306, \n",
      "- val_ndcg_10: 0.5306, \n",
      "- val_mean_mrr: 0.0673, \n",
      "- val_g_auc: 0.9859, \n",
      "\n",
      "49080/49080 [==============================] - 92s 2ms/step - loss: 0.1852 - ndcg_5: 0.4970 - ndcg_10: 0.4970 - mean_mrr: 0.0630 - g_auc: 0.9790 - val_loss: 0.0962 - val_ndcg_5: 0.5306 - val_ndcg_10: 0.5306 - val_mean_mrr: 0.0673 - val_g_auc: 0.9859\n"
     ]
    }
   ],
   "source": [
    "class CustomMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f\"\\n\\nEpoch {epoch+1}:\", end=\" \")\n",
    "        for key, value in logs.items():\n",
    "            print(f\"\\n- {key}: {value:.4f}\", end=\", \")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Using the custom callback\n",
    "custom_metrics_callback = CustomMetricsCallback()\n",
    "# Train the model\n",
    "history = content_based_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[custom_metrics_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_optimized(model, user_profiles_df, articles_df, articles_embeddings_df, k=10, num_users=2000):\n",
    "    embeddings_dict = articles_embeddings_df.T.to_dict('list')\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    ndcgs = []\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_scores = []\n",
    "\n",
    "    sampled_user_profiles_df = user_profiles_df.sample(n=num_users, random_state=42)\n",
    "    \n",
    "    for _, user in tqdm(sampled_user_profiles_df.iterrows(), total=num_users, desc=\"Evaluating\", ncols=100):\n",
    "        user_embedding = user['user_embedding']\n",
    "        user_id = user['user_id']\n",
    "        clicked_articles = set(user['click_article_id'])\n",
    "\n",
    "        all_embeddings = []\n",
    "        article_ids = []\n",
    "        for article_id in articles_df['article_id']:\n",
    "            if article_id in embeddings_dict:\n",
    "                article_embedding = embeddings_dict[article_id]\n",
    "                combined_features = np.concatenate((user_embedding, article_embedding)).reshape(1, -1)\n",
    "                all_embeddings.append(combined_features)\n",
    "                article_ids.append(article_id)\n",
    "        \n",
    "        all_embeddings = np.vstack(all_embeddings)\n",
    "        scores = model.predict(all_embeddings, verbose=0).flatten()  # Set verbose=0 to suppress model output\n",
    "        true_labels = np.array([1 if article_id in clicked_articles else 0 for article_id in article_ids])\n",
    "\n",
    "        precisions.append(precision_at_k(true_labels, scores, k))\n",
    "        recalls.append(recall_at_k(true_labels, scores, k))\n",
    "        mrrs.append(mrr(true_labels, scores))\n",
    "        ndcgs.append(ndcg_at_k(true_labels, scores, k))\n",
    "        \n",
    "        all_true_labels.extend(true_labels)\n",
    "        all_scores.extend(scores)\n",
    "\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_mrr = np.mean(mrrs)\n",
    "    avg_ndcg = np.mean(ndcgs)\n",
    "    g_auc = roc_auc_score(all_true_labels, all_scores)\n",
    "\n",
    "    return avg_ndcg, avg_mrr, avg_precision, avg_recall, g_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, y_test = prepare_training_data(user_test_profiles_df, df_articles, article_embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10: 0.4310, MRR: 0.5492, precision: 0.1400, recall: 0.4577, g_auc: 0.9786\n"
     ]
    }
   ],
   "source": [
    "# ndcg_score, mrr_score, auc_score, y_pred = evaluate_model_on_test_data(content_based_model, X_test, y_test)\n",
    "# print(f\"NDCG@10: {ndcg_score:.4f}, MRR: {mrr_score:.4f}, AUC: {auc_score:.4f}\")\n",
    "# Evaluation\n",
    "# ndcg_score, mrr_score, g_auc_score, y_true, y_pred, user_ids = evaluate_model_optimized(content_based_model, user_profiles_df_test, df_articles, article_embeddings_df, k=10, num_users=len(user_profiles_df_test))\n",
    "avg_ndcg, avg_mrr, avg_precision, avg_recall, g_auc = evaluate_model_optimized(content_based_model, user_profiles_df_test, df_articles, article_embeddings_df, k=10, num_users=10)\n",
    "print(f\"NDCG@10: {avg_ndcg:.4f}, MRR: {avg_mrr:.4f}, precision: {avg_precision:.4f}, recall: {avg_recall:.4f}, g_auc: {g_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating: 100%|███████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.13it/s]\n",
    "# NDCG@10: 0.3124, MRR: 0.3869, precision: 0.1400, recall: 0.4244, g_auc: 0.9683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_top_k_articles(user_id, user_profiles_df, df_articles, article_embeddings_df, model, k=5):\n",
    "    tmp_df_articles = df_articles.copy()\n",
    "    # Retrieve the user's embedding\n",
    "    user_profile = user_profiles_df[user_profiles_df['user_id'] == user_id].iloc[0]\n",
    "    \n",
    "    if user_profile.empty:\n",
    "        raise ValueError(\"User ID not found in the user profiles.\")\n",
    "\n",
    "    user_embedding = user_profile['user_embedding']\n",
    "\n",
    "    # Get all articles embeddings\n",
    "    embeddings_dict = article_embeddings_df.T.to_dict('list')\n",
    "    \n",
    "    article_ids = []\n",
    "    combined_features_list = []\n",
    "    \n",
    "    for article_id, article_embedding in embeddings_dict.items():\n",
    "        article_ids.append(article_id)\n",
    "        combined_features = np.concatenate((user_embedding, article_embedding)).reshape(1, -1)\n",
    "        combined_features_list.append(combined_features)\n",
    "\n",
    "    all_embeddings = np.vstack(combined_features_list)\n",
    "    \n",
    "    # Predict relevance scores using the trained model\n",
    "    scores = model.predict(all_embeddings, verbose=0).flatten()\n",
    "\n",
    "    print(user_profile[\"click_article_id\"])\n",
    "    # Add scores to dataframe\n",
    "    tmp_df_articles['score'] = tmp_df_articles['article_id'].map(dict(zip(article_ids, scores)))\n",
    "    tmp_df_articles = tmp_df_articles.sort_values(by='score', ascending=False)\n",
    "\n",
    "    top_articles = tmp_df_articles.copy()\n",
    "    top_articles = top_articles[~top_articles['article_id'].isin(user_profile[\"click_article_id\"])]\n",
    "\n",
    "    # Rank articles based on scores\n",
    "    top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "    top_k_article_ids = [article_ids[i] for i in top_k_indices]\n",
    "    \n",
    "    # Rank articles based on scores (worst)\n",
    "    bottom_k_indices = np.argsort(scores)[:k]\n",
    "    bottom_k_article_ids = [article_ids[i] for i in bottom_k_indices]\n",
    "\n",
    "    # Fetch top K articles metadata\n",
    "    top_k_articles = top_articles[top_articles['article_id'].isin(top_k_article_ids)].reset_index(drop=True)\n",
    "    bottom_k_article_ids = top_articles[top_articles['article_id'].isin(bottom_k_article_ids)].reset_index(drop=True)\n",
    "    bottom_k_article_ids = bottom_k_article_ids.sort_values(by='score', ascending=True)\n",
    "    \n",
    "    # User article clicked\n",
    "    user_article_clicked = tmp_df_articles[tmp_df_articles['article_id'].isin(user_profile[\"click_article_id\"])][[\"article_id\",\"category_id\",\"score\"]]\n",
    "    \n",
    "    # Display the top K articles usi\n",
    "    return top_k_articles, bottom_k_article_ids, user_article_clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233688, 237452, 235745, 120967]\n"
     ]
    }
   ],
   "source": [
    "user_id=3\n",
    "top_k_articles, bottom_k_article_ids, user_article_clicked = infer_top_k_articles(user_id, user_profiles_df_test, df_articles, article_embeddings_df, content_based_model, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>click_article_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235745</th>\n",
       "      <td>235745</td>\n",
       "      <td>375</td>\n",
       "      <td>0.991121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233688</th>\n",
       "      <td>233688</td>\n",
       "      <td>375</td>\n",
       "      <td>0.984925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120967</th>\n",
       "      <td>120967</td>\n",
       "      <td>249</td>\n",
       "      <td>0.978572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237452</th>\n",
       "      <td>237452</td>\n",
       "      <td>375</td>\n",
       "      <td>0.975833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  article_id  category_id     score\n",
       "click_article_id                                   \n",
       "235745                235745          375  0.991121\n",
       "233688                233688          375  0.984925\n",
       "120967                120967          249  0.978572\n",
       "237452                237452          375  0.975833"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_user_clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publisher_id</th>\n",
       "      <th>words_count</th>\n",
       "      <th>created_at_dt</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102694</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>2015-02-21 23:07:45</td>\n",
       "      <td>6.954598e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97826</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>2015-09-16 05:24:51</td>\n",
       "      <td>1.066987e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>191652</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>265</td>\n",
       "      <td>2017-02-27 21:33:02</td>\n",
       "      <td>1.449630e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100731</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>2017-07-12 14:26:44</td>\n",
       "      <td>1.544796e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185687</td>\n",
       "      <td>302</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>2017-04-02 10:16:15</td>\n",
       "      <td>1.589759e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  category_id  publisher_id  words_count       created_at_dt  \\\n",
       "4      102694          228             0          110 2015-02-21 23:07:45   \n",
       "3       97826          216             0          168 2015-09-16 05:24:51   \n",
       "2      191652          307             0          265 2017-02-27 21:33:02   \n",
       "1      100731          225             0          113 2017-07-12 14:26:44   \n",
       "0      185687          302             0          188 2017-04-02 10:16:15   \n",
       "\n",
       "          score  \n",
       "4  6.954598e-07  \n",
       "3  1.066987e-06  \n",
       "2  1.449630e-06  \n",
       "1  1.544796e-06  \n",
       "0  1.589759e-06  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_k_article_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
