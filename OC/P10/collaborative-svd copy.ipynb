{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers_2 import *\n",
    "\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from helpers_2 import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def ndcg_at_k(predictions, k=10):\n",
    "    def dcg(relevance_scores, k):\n",
    "        return sum(rel/np.log2(i+2) for i, rel in enumerate(relevance_scores[:k]))\n",
    "\n",
    "    user_predictions = {}\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        if uid not in user_predictions:\n",
    "            user_predictions[uid] = []\n",
    "        user_predictions[uid].append((true_r, est))\n",
    "    \n",
    "    ndcg_values = []\n",
    "    for uid in user_predictions:\n",
    "        user_predictions[uid].sort(key=lambda x: x[1], reverse=True)\n",
    "        relevance_scores = [pred[0] for pred in user_predictions[uid]]\n",
    "        idcg = dcg(sorted(relevance_scores, reverse=True), k)\n",
    "        dcg_value = dcg(relevance_scores, k)\n",
    "        ndcg_values.append(dcg_value/idcg if idcg > 0 else 0)\n",
    "    \n",
    "    return np.mean(ndcg_values)\n",
    "\n",
    "def mean_reciprocal_rank(predictions):\n",
    "    user_rankings = {}\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        if uid not in user_rankings:\n",
    "            user_rankings[uid] = []\n",
    "        user_rankings[uid].append((true_r, est))\n",
    "    \n",
    "    mrr_values = []\n",
    "    for uid in user_rankings:\n",
    "        user_rankings[uid].sort(key=lambda x: x[1], reverse=True)\n",
    "        for i, (true_r, est) in enumerate(user_rankings[uid]):\n",
    "            if true_r > 0:\n",
    "                mrr_values.append(1/(i+1))\n",
    "                break\n",
    "    \n",
    "    return np.mean(mrr_values)\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=4.0):\n",
    "    user_predictions = {}\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        if uid not in user_predictions:\n",
    "            user_predictions[uid] = []\n",
    "        user_predictions[uid].append((true_r, est))\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for uid in user_predictions:\n",
    "        user_predictions[uid].sort(key=lambda x: x[1], reverse=True)\n",
    "        true_positive = np.sum([1 for true_r, est in user_predictions[uid][:k] if true_r >= threshold])\n",
    "        relevant_items = np.sum([1 for true_r, est in user_predictions[uid] if true_r >= threshold])\n",
    "        recommendations_count = min(k, len(user_predictions[uid]))\n",
    "\n",
    "        precisions.append(true_positive/recommendations_count)\n",
    "        recalls.append(true_positive/relevant_items if relevant_items != 0 else 0)\n",
    "    \n",
    "    return np.mean(precisions), np.mean(recalls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have selected 503616 interactions.\n",
      "Test set length: 125904\n",
      "Train set length: 377712\n",
      "Number of predictions in Test set: 125904\n",
      "RMSE: 7.4957\n",
      "NDCG@10: 0.9803608097842309\n",
      "MRR: 1.0\n",
      "Precision@10: 0.048361442078664205\n",
      "Recall@10: 0.08253718291118296\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_articles, df_clicks = load_dataset()\n",
    "dataframe = df_clicks.merge(df_articles, left_on='click_article_id', right_on='article_id')\n",
    "dataframe = dataframe[['user_id', 'article_id', 'category_id']]\n",
    "\n",
    "series = dataframe.groupby(['user_id', 'category_id']).size()\n",
    "user_rating_matrix = series.to_frame()\n",
    "user_rating_matrix = user_rating_matrix.reset_index()\n",
    "user_rating_matrix.rename(columns={0: 'rate'}, inplace=True)\n",
    "user_rating_matrix[\"rate\"].value_counts()\n",
    "\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "_x = user_rating_matrix.loc[user_rating_matrix.rate > 1]\n",
    "data = Dataset.load_from_df(_x[['user_id', 'category_id', 'rate']], reader)\n",
    "\n",
    "print('We have selected', len(_x), 'interactions.')\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "print('Test set length:', len(testset))\n",
    "print('Train set length:', len(_x) - len(testset))\n",
    "\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "print('Number of predictions in Test set:', len(predictions))\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "ndcg10 = ndcg_at_k(predictions, k=10)\n",
    "mrr = mean_reciprocal_rank(predictions)\n",
    "precision, recall = precision_recall_at_k(predictions, k=10, threshold=7)  # Assuming rating>=7 is relevant\n",
    "\n",
    "print('NDCG@10:', ndcg10)\n",
    "print('MRR:', mrr)\n",
    "print('Precision@10:', precision)\n",
    "print('Recall@10:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
