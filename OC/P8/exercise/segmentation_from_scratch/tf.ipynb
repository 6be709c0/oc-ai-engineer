{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 16:22:58.063766: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-02-23 16:22:58.063784: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2024-02-23 16:22:58.063787: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "2024-02-23 16:22:58.063813: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-23 16:22:58.063825: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'sequential' (type Sequential).\n\nInput 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (256, 256, 4)\n\nCall arguments received by layer 'sequential' (type Sequential):\n  • inputs=tf.Tensor(shape=(256, 256, 4), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m4\u001b[39m), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m1\u001b[39m)))  \n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Train Model  \u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n",
      "Cell \u001b[0;32mIn[1], line 78\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_dataset, model, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (images, masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):  \n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:  \n\u001b[0;32m---> 78\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \n\u001b[1;32m     79\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_fn(masks, predictions)  \n\u001b[1;32m     80\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)  \n",
      "File \u001b[0;32m~/dev/6be709c0/oc-ai-engineer/OC/P8/exercise/segmentation_from_scratch/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m, in \u001b[0;36mUNET.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m skip_connections \u001b[38;5;241m=\u001b[39m []  \n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m down \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdowns:  \n\u001b[0;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     50\u001b[0m     skip_connections\u001b[38;5;241m.\u001b[39mappend(x)  \n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)  \n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mDoubleConv.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):  \n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'sequential' (type Sequential).\n\nInput 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (256, 256, 4)\n\nCall arguments received by layer 'sequential' (type Sequential):\n  • inputs=tf.Tensor(shape=(256, 256, 4), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics  \n",
    "import numpy as np  \n",
    "import os  \n",
    "import albumentations as A  \n",
    "from skimage.transform import resize  \n",
    "from skimage.io import imread, imsave  \n",
    "from tqdm import tqdm  \n",
    "  \n",
    "class DoubleConv(tf.keras.Model):  \n",
    "    def __init__(self, in_channels, out_channels):  \n",
    "        super(DoubleConv, self).__init__()  \n",
    "        self.conv = tf.keras.Sequential([  \n",
    "            layers.Conv2D(out_channels, 4, padding='same', use_bias=False),  \n",
    "            layers.BatchNormalization(),  \n",
    "            layers.ReLU(),  \n",
    "            layers.Conv2D(out_channels, 4, padding='same', use_bias=False),  \n",
    "            layers.BatchNormalization(),  \n",
    "            layers.ReLU(),  \n",
    "        ])  \n",
    "  \n",
    "    def call(self, x):  \n",
    "        return self.conv(x)  \n",
    "  \n",
    "class UNET(tf.keras.Model):  \n",
    "    def __init__(self, in_channels=4, out_channels=1, features=[64, 128, 256, 512]):  \n",
    "        super(UNET, self).__init__()  \n",
    "        self.downs = [DoubleConv(in_channels, features[0])]  \n",
    "        self.downs += [  \n",
    "            DoubleConv(features[i], features[i + 1]) for i in range(len(features) - 1)  \n",
    "        ]  \n",
    "          \n",
    "        self.ups = [  \n",
    "            DoubleConv(features[i], features[i - 1]) for i in range(len(features) - 1, 0, -1)  \n",
    "        ] + [DoubleConv(features[0] * 2, features[0])]  \n",
    "          \n",
    "        self.pool = layers.MaxPool2D(pool_size=2, strides=2)  \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)  \n",
    "        self.upconvs = [  \n",
    "            layers.Conv2DTranspose(features[i], kernel_size=2, strides=2) for i in range(len(features) - 1, -1, -1)  \n",
    "        ]  \n",
    "          \n",
    "        self.final_conv = layers.Conv2D(out_channels, kernel_size=1)  \n",
    "  \n",
    "    def call(self, x):  \n",
    "        skip_connections = []  \n",
    "          \n",
    "        for down in self.downs:  \n",
    "            x = down(x)  \n",
    "            skip_connections.append(x)  \n",
    "            x = self.pool(x)  \n",
    "              \n",
    "        x = self.bottleneck(x)  \n",
    "        skip_connections = reversed(skip_connections[:-1])  \n",
    "          \n",
    "        for idx in range(len(self.ups)):  \n",
    "            x = self.upconvs[idx](x)  \n",
    "            skip_connection = skip_connections[idx]  \n",
    "            if x.shape[1] != skip_connection.shape[1]:  \n",
    "                x = tf.image.resize(x, (skip_connection.shape[1], skip_connection.shape[2]))  \n",
    "            x = tf.concat([skip_connection, x], axis=-1)  \n",
    "            x = self.ups[idx](x)  \n",
    "  \n",
    "        return self.final_conv(x)  \n",
    "  \n",
    "# This is a simplified version and must be replaced with your actual data pipeline  \n",
    "def load_image(file_path, target_size):  \n",
    "    img = imread(file_path)  \n",
    "    img = resize(img, target_size)  \n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension  \n",
    "    return img  \n",
    "  \n",
    "def train_fn(train_dataset, model, optimizer, loss_fn, epochs=1):  \n",
    "    for epoch in range(epochs):  \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")  \n",
    "        for step, (images, masks) in enumerate(train_dataset):  \n",
    "            with tf.GradientTape() as tape:  \n",
    "                predictions = model(images, training=True)  \n",
    "                loss = loss_fn(masks, predictions)  \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)  \n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))  \n",
    "  \n",
    "            if step % 10 == 0:  \n",
    "                print(f\"Step {step}, Loss: {loss.numpy()}\")  \n",
    "  \n",
    "# This is a very simplified training loop and data loader.  \n",
    "# You'll want to replace `load_image` with your actual data loading logic.  \n",
    "  \n",
    "# Model Configuration and Initialization  \n",
    "model = UNET()  \n",
    "optimizer = optimizers.Adam()  \n",
    "loss_fn = losses.BinaryCrossentropy(from_logits=True)  \n",
    "  \n",
    "# Dummy data loader (Replace with your actual dataset)  \n",
    "# Assuming you have a function that loads your dataset images and masks.  \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((np.random.rand(10, 256, 256, 4), np.random.rand(10, 256, 256, 1)))  \n",
    "  \n",
    "# Train Model  \n",
    "train_fn(train_dataset, model, optimizer, loss_fn)  \n",
    "  \n",
    "print(\"Training complete\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
